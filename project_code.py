# -*- coding: utf-8 -*-
"""Project-code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qaYTmJPM9pYj3_Db5JE6doPKVfJRGb5R
"""

import pandas as pd
df=pd.read_csv("/content/iphone14-query-tweets.csv")

df.head()

df.tail()

import pandas as pd
import re

# Load dataset
df = pd.read_csv('/content/iphone14-query-tweets.csv')

# Sample 10,000 tweets randomly for faster processing
sampled_df = df.sample(n=10000, random_state=42).reset_index(drop=True)

# Clean tweet text but keep hashtags for event detection
def clean_tweet(text):
    text = str(text).lower()
    text = re.sub(r"http\S+|www\.\S+", "", text)  # remove URLs
    text = re.sub(r"@\w+", "", text)              # remove mentions
    text = re.sub(r"rt\s+", "", text)             # remove RT
    text = re.sub(r"[^\w\s#]", "", text)          # keep hashtags, remove punctuations
    text = re.sub(r"\s+", " ", text).strip()
    return text

sampled_df['clean_text'] = sampled_df['tweet_text'].apply(clean_tweet)

# Extract hashtags for event detection
sampled_df['hashtags'] = sampled_df['tweet_text'].apply(lambda x: re.findall(r"#\w+", str(x).lower()))

print("Sample Preview:")
print(sampled_df[['date_time', 'clean_text', 'hashtags']].head())

from collections import Counter

# Flatten all hashtags into one list
all_hashtags = [tag for sublist in sampled_df['hashtags'] for tag in sublist]
hashtag_counts = Counter(all_hashtags)

# Choose hashtags used more than a threshold (like 20 times)
top_event_hashtags = [tag for tag, count in hashtag_counts.items() if count > 20]

print("Top Event Hashtags:", top_event_hashtags)

def tag_event(hashtags):
    for tag in hashtags:
        if tag in top_event_hashtags:
            return tag  # Return the first matching event
    return None

sampled_df['event_tag'] = sampled_df['hashtags'].apply(tag_event)

def create_contextual_input(row):
    if row['event_tag']:
        return f"[EVENT: {row['event_tag']}] {row['clean_text']}"
    else:
        return row['clean_text']  # No event, fallback to just tweet text

sampled_df['bert_input'] = sampled_df.apply(create_contextual_input, axis=1)



# Sentimental analysis using Bert

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import torch.nn.functional as F

# Load model and tokenizer
model_name = "cardiffnlp/twitter-roberta-base-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# Sentiment labels for this model
labels = ['negative', 'neutral', 'positive']

def get_sentiment(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
        scores = F.softmax(outputs.logits, dim=1)
        predicted_label = torch.argmax(scores).item()
    return labels[predicted_label]

# Run sentiment analysis on BERT inputs
sampled_df['sentiment'] = sampled_df['bert_input'].apply(get_sentiment)

# Temporal Analysis of tweets # positive negative and neutral

import matplotlib.pyplot as plt

# Convert date column to datetime
sampled_df['date'] = pd.to_datetime(sampled_df['date_time']).dt.date

# Group by date and sentiment
sentiment_timeline = sampled_df.groupby(['date', 'sentiment']).size().unstack(fill_value=0)

# Normalize (optional)
sentiment_timeline_norm = sentiment_timeline.div(sentiment_timeline.sum(axis=1), axis=0)

# Plot
plt.figure(figsize=(12, 6))
sentiment_timeline_norm.plot(kind='line', marker='o', colormap='coolwarm', figsize=(14,6))
plt.title("Sentiment Trend Over Time")
plt.xlabel("Date")
plt.ylabel("Proportion of Sentiment")
plt.grid(True)
plt.tight_layout()
plt.show()

# Temporal analysis of sentiments.

import pandas as pd
import matplotlib.pyplot as plt

# Assign sentiment scores
sentiment_map = {'positive': 1, 'neutral': 0, 'negative': -1}
sampled_df['sentiment_score'] = sampled_df['sentiment'].map(sentiment_map)

# Extract date and calculate daily average sentiment score
sampled_df['date'] = pd.to_datetime(sampled_df['date_time']).dt.date
daily_sentiment = sampled_df.groupby('date')['sentiment_score'].mean().reset_index()

# Plotting a single line for sentiment trend
plt.figure(figsize=(12, 6))
plt.plot(daily_sentiment['date'], daily_sentiment['sentiment_score'], color='purple', marker='o')
plt.title("Average Sentiment Over Time")
plt.xlabel("Date")
plt.ylabel("Average Sentiment Score")
plt.xticks(rotation=45)
plt.grid(True)
plt.tight_layout()
plt.show()

# no what causes the sudden spikes

"""hastags or any other factors that caused sudden spike

"""

# Calculate daily change
daily_sentiment['sentiment_diff'] = daily_sentiment['sentiment_score'].diff()

# Set threshold for what counts as a spike/dip
spike_threshold = 0.3

# Flag spikes and dips
spikes = daily_sentiment[daily_sentiment['sentiment_diff'] > spike_threshold]
dips = daily_sentiment[daily_sentiment['sentiment_diff'] < -spike_threshold]

spike_dates = spikes['date'].tolist()
dip_dates = dips['date'].tolist()

# Use 'sampled_df' instead of 'tweet_df'
spike_tweets = sampled_df[sampled_df['date'].isin(spike_dates)]
dip_tweets = sampled_df[sampled_df['date'].isin(dip_dates)]

from collections import Counter
import re

def extract_hashtags(text):
    return re.findall(r"#\w+", text)

# Combine spike and dip tweets
all_event_tweets = pd.concat([spike_tweets, dip_tweets])
# Change 'full_text' to 'tweet_text' which is the column with the actual tweet content
all_event_tweets['hashtags'] = all_event_tweets['tweet_text'].apply(extract_hashtags)

hashtag_counts = Counter([tag for tags in all_event_tweets['hashtags'] for tag in tags])
top_hashtags = hashtag_counts.most_common(15)

print(top_hashtags)



def hashtags_on_date(df, target_date):
    tweets = df[df['date'] == target_date]
    hashtags = tweets['tweet_text'].apply(extract_hashtags) # Using 'tweet_text' column
    flat_hashtags = [tag for tags in hashtags for tag in tags]
    return Counter(flat_hashtags).most_common(10)

# Example: Analyze hashtags on the biggest sentiment spike
top_spike_date = spikes.sort_values(by='sentiment_diff', ascending=False)['date'].iloc[0]
top_spike_hashtags = hashtags_on_date(sampled_df, top_spike_date) # Using 'sampled_df'
print(f"Top hashtags on {top_spike_date}:", top_spike_hashtags)



"""Now i want to find out which hastags lead to postive and negative spikes"""

# Multi aspect sentimental analysis

aspects = {
    "design": ["design", "look", "aesthetic", "style"],
    "price": ["price", "cost", "expensive", "cheap"],
    "features": ["feature", "spec", "specs", "function"],
    "performance": ["performance", "speed", "lag", "smooth"],
    "battery": ["battery", "charging", "power"],
    "camera": ["camera", "photo", "selfie", "lens"]
}

def tag_aspect(text):
    text = text.lower()
    for aspect, keywords in aspects.items():
        if any(keyword in text for keyword in keywords):
            return aspect
    return "other"

sampled_df["aspect"] = sampled_df["clean_text"].apply(tag_aspect)

# Convert the 'date' column to datetime objects if it's not already
sampled_df['date'] = pd.to_datetime(sampled_df['date'])

sentiment_by_aspect = (
    sampled_df.groupby([sampled_df['date'].dt.date, "aspect"])
    .agg(avg_sentiment=("sentiment_score", "mean"))
    .reset_index()
)



import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12, 6))
sns.lineplot(data=sentiment_by_aspect, x="date", y="avg_sentiment", hue="aspect")
plt.title("Multi-Aspect Sentiment Trend Over Time")
plt.ylabel("Average Sentiment")
plt.xlabel("Date")
plt.xticks(rotation=45)
plt.legend(title="Aspect")
plt.grid(True)
plt.tight_layout()
plt.show()

print(sampled_df)

# now each and every aspect will be calculated

aspects = {
    "design": ["design", "look", "aesthetic", "style"],
    "price": ["price", "cost", "expensive", "cheap"],
    "features": ["feature", "spec", "specs", "function"],
    "performance": ["performance", "speed", "lag", "smooth"],
    "battery": ["battery", "charging", "power"],
    "camera": ["camera", "photo", "selfie", "lens"]
}

def tag_aspect(text):
    text = text.lower()
    for aspect, keywords in aspects.items():
        if any(keyword in text for keyword in keywords):
            return aspect
    return "other"

sampled_df["aspect"] = sampled_df["clean_text"].apply(tag_aspect)
sampled_df["date"] = pd.to_datetime(sampled_df["date_time"]).dt.date  # ensure date format

sentiment_by_aspect = (
    sampled_df.groupby(["date", "aspect"])
    .agg(avg_sentiment=("sentiment_score", "mean"))
    .reset_index()
)

import matplotlib.pyplot as plt
import seaborn as sns

aspects_to_plot = sentiment_by_aspect["aspect"].unique()

for aspect in aspects_to_plot:
    if aspect == "other":
        continue  # skip unrelated tweets

    aspect_data = sentiment_by_aspect[sentiment_by_aspect["aspect"] == aspect]

    plt.figure(figsize=(10, 4))
    sns.lineplot(data=aspect_data, x="date", y="avg_sentiment", marker='o')
    plt.title(f"Sentiment Trend Over Time: {aspect.capitalize()}")
    plt.xlabel("Date")
    plt.ylabel("Average Sentiment")
    plt.ylim(-1, 1)  # scale for consistency
    plt.grid(True)
    plt.tight_layout()
    plt.show()

# Save to a new CSV file
sampled_df.to_csv('iphone14-query-tweets_analyzed50.csv', index=False)

df=pd.read_csv("/content/iphone14-query-tweets_analyzed50.csv")

#Apply LDA to get the topics

import pandas as pd
import re
import nltk
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

nltk.download('stopwords')
from nltk.corpus import stopwords

# Assume sampled_df is already loaded
df=pd.read_csv("/content/iphone14-query-tweets_analyzed50.csv")

# Text cleaning function
def preprocess(text):
    text = str(text).lower()
    text = re.sub(r"http\S+|@\w+|#\w+|[^a-z\s]", "", text)
    return text

# Clean tweets
df['clean_text'] = df['tweet_text'].apply(preprocess)

# Vectorize text
vectorizer = CountVectorizer(stop_words='english', max_df=0.95, min_df=10)
dtm = vectorizer.fit_transform(df['clean_text'])

# Apply LDA
lda = LatentDirichletAllocation(n_components=5, random_state=42)
lda.fit(dtm)

# Print top words for each topic
def display_topics(model, feature_names, no_top_words):
    for topic_idx, topic in enumerate(model.components_):
        top_words = [feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]
        print(f"Topic {topic_idx + 1}: {', '.join(top_words)}")

print("Top words per topic:")
display_topics(lda, vectorizer.get_feature_names_out(), 10)

import pandas as pd
import re
import nltk
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

nltk.download('stopwords')
from nltk.corpus import stopwords

# Load the dataframe that contains the 'sampled_df' variable
df = pd.read_csv("/content/iphone14-query-tweets_analyzed50.csv")

# Text cleaning function
def preprocess(text):
    text = str(text).lower()
    text = re.sub(r"http\S+|@\w+|#\w+|[^a-z\s]", "", text)
    return text

# Clean tweets
df['clean_text'] = df['tweet_text'].apply(preprocess)

# Vectorize text
vectorizer = CountVectorizer(stop_words='english', max_df=0.95, min_df=10)
dtm = vectorizer.fit_transform(df['clean_text'])

# Apply LDA
lda = LatentDirichletAllocation(n_components=5, random_state=42)
lda.fit(dtm)

# Get dominant topic for each document
topic_probs = lda.transform(dtm)
df['dominant_topic'] = topic_probs.argmax(axis=1) +1 #add 1 to avoid topic 0


# Now you can use 'sampled_df' in your analysis:
topic_sentiment = df.groupby(['dominant_topic', 'date'])['sentiment_score'].mean().unstack(0)

plt.figure(figsize=(12, 6))
for col in topic_sentiment.columns:
    plt.plot(topic_sentiment.index, topic_sentiment[col], label=f'Topic {col}')
plt.xlabel('Date')
plt.ylabel('Average Sentiment Score')
plt.title('Sentiment Trends Over Time for Each Topic')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt



# Define your aspect mapping
topic_to_aspect = {
    0: "Market & News",
    1: "Product Features",
    2: "Purchase Intent",
    3: "Pricing & Variants",
    4: "Launch Event"
}

# Convert date column
df['date_time'] = pd.to_datetime(df['date_time'])
df['date'] = df['date_time'].dt.date

# Map topic to aspect
df['aspect'] = df['dominant_topic'].map(topic_to_aspect)

# Group by date and aspect
grouped = df.groupby(['date', 'aspect'])['sentiment_score'].mean().unstack()

# Plot
plt.figure(figsize=(14, 7))
for aspect in grouped.columns:
    plt.plot(grouped.index, grouped[aspect], label=aspect)

plt.xlabel('Date')
plt.ylabel('Average Sentiment Score')
plt.title('Sentiment Over Time by Aspect')
plt.xticks(rotation=45)
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

#hashtags analysis

#Extract hashtags and prepare data

import pandas as pd
import re
from collections import defaultdict
import matplotlib.pyplot as plt



# Function to extract hashtags
def extract_hashtags(text):
    return re.findall(r"#\w+", str(text).lower())

# Create a new column for hashtags list
df["hashtags"] = df["tweet_text"].apply(extract_hashtags)

# Explode hashtags for analysis
df_exploded = df.explode("hashtags")
df_exploded = df_exploded.dropna(subset=["hashtags"])

#Aggregate sentiments per hashtag

# Group by hashtag
hashtag_sentiment = df_exploded.groupby("hashtags")["sentiment_score"].agg(['count', 'mean']).reset_index()

# Rename columns
hashtag_sentiment.columns = ["hashtag", "count", "avg_sentiment"]

# Filter hashtags that appear more than 10 times to reduce noise
filtered_hashtags = hashtag_sentiment[hashtag_sentiment['count'] > 10]

# frequent hashtags

top_impactful = filtered_hashtags.sort_values("count", ascending=False).head(10)

plt.figure(figsize=(10, 6))
plt.barh(top_impactful["hashtag"], top_impactful["count"], color='skyblue')
plt.xlabel("Frequency")
plt.title("Top 10 Most Frequent Hashtags")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

#top positive hashtags

top_positive = filtered_hashtags.sort_values("avg_sentiment", ascending=False).head(10)

plt.figure(figsize=(10, 6))
plt.barh(top_positive["hashtag"], top_positive["avg_sentiment"], color='green')
plt.xlabel("Average Sentiment Score")
plt.title("Top 10 Hashtags with Positive Sentiment")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

#Top negative hashtags

top_negative = filtered_hashtags.sort_values("avg_sentiment").head(10)

plt.figure(figsize=(10, 6))
plt.barh(top_negative["hashtag"], top_negative["avg_sentiment"], color='red')
plt.xlabel("Average Sentiment Score")
plt.title("Top 10 Hashtags with Negative Sentiment")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()



# Flatten and clean all hashtags
all_hashtags = [tag.lower() for tags in df['hashtags'] if isinstance(tags, list) for tag in tags]

# Filter hashtags containing the word 'launch'
launch_related = [tag for tag in all_hashtags if 'launch' in tag]

# Get frequency count
from collections import Counter
launch_hashtag_counts = Counter(launch_related)

# Display the most common ones
print("Launch-related hashtags:")
for tag, count in launch_hashtag_counts.most_common():
    print(f"{tag}: {count}")

